#!/bin/bash
#SBATCH --begin=now+8hour
#SBATCH --spread-job 
#SBATCH --job-name=parallel_all       ### name your job
#SBATCH --time=10:00:00            
#SBATCH --ntasks=1                
#SBATCH --cpus-per-task=1           ### every tasks has 2 threads
#SBATCH --mem-per-cpu=32000
#SBATCH --partition=gpu
#SBATCH --nodelist=ge01
#SBATCH --gres=gpu:K80:3

# To receive an email when job completes or fails
#SBATCH --mail-user=tinplay41@gmail.com
#SBATCH --mail-type=ALL



#SBATCH --output=conv_lstm_.%j.out

module load pytorch/1.5-cuda10

srun python3 parallel.py --num_epochs 200 --model_name parallel_all
exit 0
